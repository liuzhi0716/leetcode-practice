1、HashMap 和 Hashtable 区别
    安全性
    null是否可以作为key和value的值
    fail-fast
    父类: Dictionary / Map
    数组初始化大小: 11 / 16


2、Java 垃圾回收机制和生命周期
    内存模型
    回收算法
    如何确认垃圾是否可回收
    垃圾回收器


3、怎么解决 Kafka 数据丢失的问题
    生产端数据丢失
        重试
        ack=all
    kafka端数据丢失
        topic设置replication.factor 多副本
        kafka服务端设置min.insync.replicas 感知follower数
        producer端设置ack=all
        producer端设置retries=MAX
    消费端数据丢失
        自动提交offset改为手动提交
        重复消费问题：业务端保证幂等性


4、zookeeper 是如何保证数据一致性的
    一致性算法：zab, raft, paxos(basic paxos, multi paxos)
        实例详解ZooKeeper ZAB协议、分布式锁与领导选举: https://dbaplus.cn/news-141-1875-1.html
        比较raft ，basic paxos以及multi-paxos: https://zhuanlan.zhihu.com/p/25664121
            脑裂问题，多主问题（网络分区）？
            活锁问题，频繁选举？
            幽灵复现？

            term值，版本号
            租约，快过期直接续约
            随机时间开始选主


5、hadoop 和 spark 在处理数据时，处理出现内存溢出的方法有
    map过程产生大量对象导致内存溢出
        map之前调用repartition方法
    数据不平衡导致内存溢出
        repartition重新分区
    coalesce调用导致内存溢出
        机制不同于repartition
    shuffle后内存溢出
        增加spark.default.parallelism
    standalone模式下资源分配不均与导致内存溢出
        配置spark.executor.cores参数，确保Executor资源分配均匀
        spark.cores.max 是指你的spark程序需要的总核数
            executor 数量 = spark.cores.max/spark.executor.cores
        spark.storage.memoryFraction
            该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。
        spark.shuffle.memoryFraction
            该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。
        RDD中，利用常量池共用对象减少OOM的情况
            new String("a"+"b") 改为 "a"+"b，字符串"ab"会存在方法区，即常量池


6、java 实现快速排序
7、设计微信群发红包数据库表结构（包含表名称、字段名称、类型）
8、如何选型：
    业务场景、性能要求、维护和扩展性、成本、开源活跃度


9、Spark如何调优
    使用foreachPartitions替代foreach
        mapPartitions替换map：一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。
    设置num-executors参数（spark.executor.instances）：该参数用于设置Spark作业总共要用多少个Executor进程来执行。
    设置executor-memory参数：该参数用于设置每个Executor进程的内存。
    executor-cores：该参数用于设置每个Executor进程的CPU core数量。
    driver-memory：该参数用于设置Driver进程的内存。
    spark.default.parallelism：该参数用于设置每个stage的默认task数量。
        Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。
        该参数为num-executors * executor-cores的2~3倍较为合适
    spark.storage.memoryFraction：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。
    spark.shuffle.memoryFraction：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。


10、Flink和spark的通信框架有什么异同



11、Java的代理
12、Java的内存溢出和内存泄漏
13、hadoop 的组件有哪些？Yarn的调度器有哪些？
14、hadoop 的 shuffle 过程
15、简述Spark集群运行的几种模式
16、RDD 中的 reducebyKey 与 groupByKey 哪个性能高？
17、简述 HBase 的读写过程 
18、在 2.5亿个整数中，找出不重复的整数，注意：内存不足以容纳 2.5亿个整数。
19、CDH 和 HDP 的区别 
20、Java原子操作

 
21、Java封装、继承和多态 
22、JVM 模型 
23、Flume taildirSorce 重复读取数据解决方法 
24、Flume 如何保证数据不丢 
25、Java 类加载过程 
26、Spark Task 运行原理 
27、手写一个线程安全的单例 
28、设计模式 
29、impala 和 kudu 的适用场景，读写性能如何 
30、Kafka ack原理 
31、phoenix 创建索引的方式及区别 
32、Flink TaskManager 和 Job Manager 通信 
33、Flink 双流 join方式 
34、Flink state 管理和 checkpoint 的流程 
35、Flink 分层架构 
36、Flink 窗口 
37、Flink watermark 如何处理乱序数据 
38、Flink time 
39、Flink支持exactly-once 的 sink 和 source 
40、Flink 提交作业的流程 
41、Flink connect 和 join 区别 
42、重启 task 的策略 
43、hive 的锁 
44、hive sql 优化方式 
45、hadoop shuffle 过程和架构 
46、如何优化 shuffle过程 
47、冒泡排序和快速排序 
48、讲讲Spark的stage 
49、spark mkrdd和Parrallilaze函数区别 
50、Spark checkpoint 过程 
51、二次排序 
52、如何注册 hive udf 
53、SQL去重方法 
54、Hive分析和窗口函数 
55、Hadoop 容错，一个节点挂掉然后又上线 
56、掌握 JVM 原理 
57、Java 并发原理 
58、多线程的实现方法 
59、RocksDBStatebackend实现（源码级别） 
60、HashMap、ConcurrentMap和 Hashtable 区别 
61、Flink Checkpoint 是怎么做的，作用到算子还是chain 
62、Checkpoint失败了的监控 
63、String、StringBuffer和 StringBuilder的区别 
64、Kafka存储流程，为什么高吞吐？
65、Spark优化方法举例 
66、keyby的最大并行度 
67、Flink 优化方法 
68、Kafka ISR 机制 
69、Kafka partition的4个状态 
70、Kafka 副本的7个状态 
71、Flink taskmanager的数量 
72、if 和 switch 的性能及 switch 支持的参数 
73、kafka 零拷贝 
74、hadoop 节点容错机制 
75、HDFS 的副本分布策略 
76、Hadoop面试题汇总，大概都在这里(https://www.cnblogs.com/gala1021/p/8552850.html) 
77、Kudu 和Impala 权限控制 
78、Time_wait状态？当server处理完client的请求后立刻closesocket此时会出现time_wait状态
79、三次握手交换了什么？(SYN,ACK,SEQ,窗口大小) 3次握手建立链接，4次握手断开链接。
80、hashmap 1.7和1.8 的区别 
81、concurrenthashmap 1.7和1.8？
82、Kafka 的ack 
83、sql 去重方法(group by 、distinct、窗口函数) 
84、哪些 Hive sql 不能在 Spark sql 上运行，看这里：https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#unsupported-hive-functionality 
85、什么情况下发生死锁 
86、事务隔离级别？可重复读、不可重复读、读未提交、串行化 
87、Spark shuffle 和 Hadoop shuffle的异同 
88、Spark静态内存和动态内存 
89、mysql btree 和 hash tree 的区别。btree 需要唯一主键，hash tree 适合>= 等，精确匹配，不适合范围检索 
90、udf、udtf和 udaf 的区别 
91、hive sql 的执行过程 
92、spark sql 的执行过程 
93、找出数组中最长的top10字符串 
94、Flink 数据处理流程 
95、Flink 与 Spark streaming 对比 
96、Flink watermark 使用 
97、窗口与流的结合 
98、Flink 实时告警设计 
99、Java：面向对象、容器、多线程、单例 
100、Flink：部署、API、状态、checkpoint、savepoint、watermark、重启策略、datastream 算子和优化、job和task状态 
101、Spark：原理、部署、优化 
102、Kafka：读写原理、使用、优化 
103、hive的外部表 
104、spark的函数式编程 
105、线性数据结构和数据结构 
106、Spark映射，RDD
107、java的内存溢出和内存泄漏
108、多线程的实现方法 
109、HashMap、ConcurrentMap和 Hashtable 区别 
110、Flink Checkpoint 是怎么做的，作用到算子还是chain 
111、Checkpoint失败了的监控 
112、String、StringBuffer和 StringBuilder的区别 
113、Kafka存储流程，为什么高吞吐 
114、Spark 优化方法举例 
115、keyby 的最大并行度 
116、Flink 优化方法 
117、Kafka ISR 机制 
118、kafka partition 的状态 
119、kafka 副本的状态 
120、taskmanager 的数量 
121、if 和switch的性能区别
122、Hdfs读写流程（结合cap理论讲） 
123、技术选型原则 
124、Kafka组件介绍 
125、g1和cms的区别 
126、讲讲最熟悉的数据结构 
127、spark oom处理方法 
128、看了哪些源码 
129、Spark task原理 
130、解决过的最有挑战的问题 
131、Hbase读写流程
